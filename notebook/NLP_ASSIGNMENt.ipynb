{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!pip install nltk\n",
        "!pip install nltk scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ovFMXwFUd35",
        "outputId": "0db2f5c4-de7a-4915-acb6-51ca3a400478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the req libraries\n",
        "import nltk\n",
        "import re # Import the regular expression module\n",
        "from nltk.tokenize import word_tokenize ##TOKENZIE LIB\n",
        "from nltk.corpus import stopwords #USED FOR STOPWORDS\n",
        "import string\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize ##importing libraries\n",
        "from nltk import ngrams\n",
        "nltk.download ('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjDNIcG1Ud1q",
        "outputId": "4bc37f9b-1eb0-462f-d673-5b8eeccd82a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "1OUMbPAvTh8T",
        "outputId": "b4e490c4-4922-42f2-f623-80d1d9a22e74"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-18ed26bf-ad29-4556-93fc-f3c23f206d8f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-18ed26bf-ad29-4556-93fc-f3c23f206d8f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"abdullah13554\",\"key\":\"d3b644a321493fbcdb0fb0d358c6e852\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()    # upload kaggle.json here\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "shutil.move(\"kaggle.json\", \"/root/.kaggle/kaggle.json\")\n",
        "os.chmod(\"/root/.kaggle/kaggle.json\", 600)"
      ],
      "metadata": {
        "id": "3QhvfEPhTpXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews --unzip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRL4NgHpT6vt",
        "outputId": "30e0cea3-5dcc-4b60-83e7-4136533df007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
            "License(s): other\n",
            "Downloading imdb-dataset-of-50k-movie-reviews.zip to /content\n",
            "  0% 0.00/25.7M [00:00<?, ?B/s]\n",
            "100% 25.7M/25.7M [00:00<00:00, 1.51GB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"IMDB Dataset.csv\")  # change filename if it's different\n",
        "print(df.head())\n",
        "print(df.info())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAOh09p2T6tZ",
        "outputId": "25fd5241-9d2b-4ad1-f377-b59756a8479d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     50000 non-null  object\n",
            " 1   sentiment  50000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 781.4+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**stemming and lemmatization**"
      ],
      "metadata": {
        "id": "KDgdfuaI0zWV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXA_AjhEUgvB",
        "outputId": "aae043c7-cc2d-467a-e55d-cff81314505e"
      },
      "source": [
        "text = df['review'][0] # Get the first review as a string\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word.lower()) for word in word_tokenize(text)]\n",
        "print(\"Original Sentence (first review): \", text)\n",
        "print(\"Stemmed Words:\", stemmed_words)\n",
        "\n",
        "\n",
        "##output of stemming and lemma using SAMPLE sentences\n",
        "# using samlpe text for stemmer\n",
        "text = \" The emoji was predated by the emoticon,[14] a concept implemented in 1982 by computer scientist Scott Fahlman when he suggested text-based symbols such as :-) and :-( could be used to replace language.\"\n",
        "stemmer=PorterStemmer()\n",
        "stemmed_words=[stemmer.stem(word.lower()) for word in word_tokenize(text)]\n",
        "print(\"\\nOriginal Sentence\\n:\", text)\n",
        "print(\"\\nStemmed Words\\n:\", stemmed_words)\n",
        "\n",
        "##using it for lemmatizer\n",
        "lemmatizer= WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(text)]\n",
        "print(\"\\nLemmatized Words:\", lemmatized_words)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence (first review):  One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n",
            "Stemmed Words: ['one', 'of', 'the', 'other', 'review', 'ha', 'mention', 'that', 'after', 'watch', 'just', '1', 'oz', 'episod', 'you', \"'ll\", 'be', 'hook', '.', 'they', 'are', 'right', ',', 'as', 'thi', 'is', 'exactli', 'what', 'happen', 'with', 'me.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'wa', 'it', 'brutal', 'and', 'unflinch', 'scene', 'of', 'violenc', ',', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', '.', 'trust', 'me', ',', 'thi', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'heart', 'or', 'timid', '.', 'thi', 'show', 'pull', 'no', 'punch', 'with', 'regard', 'to', 'drug', ',', 'sex', 'or', 'violenc', '.', 'it', 'is', 'hardcor', ',', 'in', 'the', 'classic', 'use', 'of', 'the', 'word.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'it', 'is', 'call', 'oz', 'as', 'that', 'is', 'the', 'nicknam', 'given', 'to', 'the', 'oswald', 'maximum', 'secur', 'state', 'penitentari', '.', 'it', 'focus', 'mainli', 'on', 'emerald', 'citi', ',', 'an', 'experiment', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cell', 'have', 'glass', 'front', 'and', 'face', 'inward', ',', 'so', 'privaci', 'is', 'not', 'high', 'on', 'the', 'agenda', '.', 'em', 'citi', 'is', 'home', 'to', 'mani', '..', 'aryan', ',', 'muslim', ',', 'gangsta', ',', 'latino', ',', 'christian', ',', 'italian', ',', 'irish', 'and', 'more', '....', 'so', 'scuffl', ',', 'death', 'stare', ',', 'dodgi', 'deal', 'and', 'shadi', 'agreement', 'are', 'never', 'far', 'away.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goe', 'where', 'other', 'show', 'would', \"n't\", 'dare', '.', 'forget', 'pretti', 'pictur', 'paint', 'for', 'mainstream', 'audienc', ',', 'forget', 'charm', ',', 'forget', 'romanc', '...', 'oz', 'doe', \"n't\", 'mess', 'around', '.', 'the', 'first', 'episod', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasti', 'it', 'wa', 'surreal', ',', 'i', 'could', \"n't\", 'say', 'i', 'wa', 'readi', 'for', 'it', ',', 'but', 'as', 'i', 'watch', 'more', ',', 'i', 'develop', 'a', 'tast', 'for', 'oz', ',', 'and', 'got', 'accustom', 'to', 'the', 'high', 'level', 'of', 'graphic', 'violenc', '.', 'not', 'just', 'violenc', ',', 'but', 'injustic', '(', 'crook', 'guard', 'who', \"'ll\", 'be', 'sold', 'out', 'for', 'a', 'nickel', ',', 'inmat', 'who', \"'ll\", 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', ',', 'well', 'manner', ',', 'middl', 'class', 'inmat', 'be', 'turn', 'into', 'prison', 'bitch', 'due', 'to', 'their', 'lack', 'of', 'street', 'skill', 'or', 'prison', 'experi', ')', 'watch', 'oz', ',', 'you', 'may', 'becom', 'comfort', 'with', 'what', 'is', 'uncomfort', 'view', '....', 'that', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side', '.']\n",
            "\n",
            "Original Sentence\n",
            ":  The emoji was predated by the emoticon,[14] a concept implemented in 1982 by computer scientist Scott Fahlman when he suggested text-based symbols such as :-) and :-( could be used to replace language.\n",
            "\n",
            "Stemmed Words\n",
            ": ['the', 'emoji', 'wa', 'predat', 'by', 'the', 'emoticon', ',', '[', '14', ']', 'a', 'concept', 'implement', 'in', '1982', 'by', 'comput', 'scientist', 'scott', 'fahlman', 'when', 'he', 'suggest', 'text-bas', 'symbol', 'such', 'as', ':', '-', ')', 'and', ':', '-', '(', 'could', 'be', 'use', 'to', 'replac', 'languag', '.']\n",
            "\n",
            "Lemmatized Words: ['the', 'emoji', 'wa', 'predated', 'by', 'the', 'emoticon', ',', '[', '14', ']', 'a', 'concept', 'implemented', 'in', '1982', 'by', 'computer', 'scientist', 'scott', 'fahlman', 'when', 'he', 'suggested', 'text-based', 'symbol', 'such', 'a', ':', '-', ')', 'and', ':', '-', '(', 'could', 'be', 'used', 'to', 'replace', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words=[lemmatizer.lemmatize(word.lower()) for word in word_tokenize(text)]\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dbaa5vjpaPRZ",
        "outputId": "afc2721f-60c0-4e15-e8ea-2c2c19d54559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words: ['the', 'emoji', 'wa', 'predated', 'by', 'the', 'emoticon', ',', '[', '14', ']', 'a', 'concept', 'implemented', 'in', '1982', 'by', 'computer', 'scientist', 'scott', 'fahlman', 'when', 'he', 'suggested', 'text-based', 'symbol', 'such', 'a', ':', '-', ')', 'and', ':', '-', '(', 'could', 'be', 'used', 'to', 'replace', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing stopwords, punctuation, special characters"
      ],
      "metadata": {
        "id": "zyZCI19t06EV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3A7faot3gfJ",
        "outputId": "85e290c0-0f7f-4083-c955-cfc192e7b15c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " \"he's\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " \"i've\",\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " \"we've\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize Text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "# Remove Stopwords\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "\n",
        "print(\"Original Tokens:\", tokens)\n",
        "print(\"Filtered Tokens (Without Stopwords):\", filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwBE-A3O2p1M",
        "outputId": "62fde0de-41f3-4fb0-bcf1-30f59ecc035e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens: ['The', 'emoji', 'was', 'predated', 'by', 'the', 'emoticon', ',', '[', '14', ']', 'a', 'concept', 'implemented', 'in', '1982', 'by', 'computer', 'scientist', 'Scott', 'Fahlman', 'when', 'he', 'suggested', 'text-based', 'symbols', 'such', 'as', ':', '-', ')', 'and', ':', '-', '(', 'could', 'be', 'used', 'to', 'replace', 'language', '.']\n",
            "Filtered Tokens (Without Stopwords): ['emoji', 'predated', 'emoticon', ',', '[', '14', ']', 'concept', 'implemented', '1982', 'computer', 'scientist', 'Scott', 'Fahlman', 'suggested', 'text-based', 'symbols', ':', '-', ')', ':', '-', '(', 'could', 'used', 'replace', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    ## Normalize convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and special characters\n",
        "\n",
        "    text = re.sub(r'[^À-ſ\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "## APPLYING NORMALIZATION\n",
        "df['normalized_review'] = df['review'].apply(preprocess_text)\n",
        "print(\"orignal review\")\n",
        "print(df['review'][0])\n",
        "print(\"normalized review\")\n",
        "print(df['normalized_review'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgXC-1YY3pv4",
        "outputId": "aa32ce8f-9177-42df-eca7-9565cd12a25d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orignal review\n",
            "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n",
            "normalized review\n",
            "one of the other reviewers has mentioned that after watching just 1 oz episode youll be hooked they are right as this is exactly what happened with mebr br the first thing that struck me about oz was its brutality and unflinching scenes of violence which set in right from the word go trust me this is not a show for the faint hearted or timid this show pulls no punches with regards to drugs sex or violence its is hardcore in the classic use of the wordbr br it is called oz as that is the nickname given to the oswald maximum security state penitentary it focuses mainly on emerald city an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda em city is home to manyaryans muslims gangstas latinos christians italians irish and moreso scuffles death stares dodgy dealings and shady agreements are never far awaybr br i would say the main appeal of the show is due to the fact that it goes where other shows wouldnt dare forget pretty pictures painted for mainstream audiences forget charm forget romanceoz doesnt mess around the first episode i ever saw struck me as so nasty it was surreal i couldnt say i was ready for it but as i watched more i developed a taste for oz and got accustomed to the high levels of graphic violence not just violence but injustice crooked guards wholl be sold out for a nickel inmates wholl kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience watching oz you may become comfortable with what is uncomfortable viewingthats if you can get in touch with your darker side\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. N-gram Language Models**\n",
        "\n",
        "• Generate unigrams, bigrams, and trigrams\n",
        "\n",
        "• Analyze how N-grams change sentence representation\n",
        "\n",
        "• Build a vocabulary based on N-grams\n",
        "\n",
        "• Calculate simple N-gram probabilities for a small text sample"
      ],
      "metadata": {
        "id": "hns5l1F4xVbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['review'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajuuda2R3ptw",
        "outputId": "f303c84b-2311-43b1-b223-312c0a908ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        One of the other reviewers has mentioned that ...\n",
            "1        A wonderful little production. <br /><br />The...\n",
            "2        I thought this was a wonderful way to spend ti...\n",
            "3        Basically there's a family where a little boy ...\n",
            "4        Petter Mattei's \"Love in the Time of Money\" is...\n",
            "                               ...                        \n",
            "49995    I thought this movie did a down right good job...\n",
            "49996    Bad plot, bad dialogue, bad acting, idiotic di...\n",
            "49997    I am a Catholic taught in parochial elementary...\n",
            "49998    I'm going to have to disagree with the previou...\n",
            "49999    No one expects the Star Trek movies to be high...\n",
            "Name: review, Length: 50000, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = df['normalized_review'].tolist() # Get reviews as a list of strings\n",
        "\n",
        "# Tokenizing sentences into words\n",
        "tokens = [nltk.word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "flat_tokens = [item for sublist in tokens for item in sublist]"
      ],
      "metadata": {
        "id": "kLq7VHaqyRFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##generating unigrams\n",
        "unigrams_list = list(ngrams(flat_tokens, 1))\n",
        "print(\"First 10 Unigrams:\", unigrams_list[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elSoOi4AyvhQ",
        "outputId": "551b9156-6f72-424b-ed3c-112213a54c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 Unigrams: [('one',), ('of',), ('the',), ('other',), ('reviewers',), ('has',), ('mentioned',), ('that',), ('after',), ('watching',)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#gen bigrams\n",
        "bigrams_list = list(ngrams(flat_tokens, 2))\n",
        "print(\"First 10 Bigrams:\", bigrams_list[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIBtc73bzASe",
        "outputId": "8900c378-e114-46f3-e4b6-c5f9e3fcd003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 Bigrams: [('one', 'of'), ('of', 'the'), ('the', 'other'), ('other', 'reviewers'), ('reviewers', 'has'), ('has', 'mentioned'), ('mentioned', 'that'), ('that', 'after'), ('after', 'watching'), ('watching', 'just')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##gen trigrams\n",
        "trigrams_list=list(ngrams(flat_tokens,3))\n",
        "print (\"first 10 trigrams\",trigrams_list[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9B2EV5Mbztyp",
        "outputId": "17676acc-9620-4282-f05c-0f5bcf7cd739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first 10 trigrams [('one', 'of', 'the'), ('of', 'the', 'other'), ('the', 'other', 'reviewers'), ('other', 'reviewers', 'has'), ('reviewers', 'has', 'mentioned'), ('has', 'mentioned', 'that'), ('mentioned', 'that', 'after'), ('that', 'after', 'watching'), ('after', 'watching', 'just'), ('watching', 'just', '1')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzing how ngrams change sentence represntion"
      ],
      "metadata": {
        "id": "9k8bDxzA1-aG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review=df\n",
        "print(df['review'],review[:0])\n",
        "\n",
        "print(\"\\nfirst 10 unigrams of the first sentence\\n\",unigrams_list[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4tuQY7X184v",
        "outputId": "1134b3ec-9e25-4466-ce4b-3b881336e166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        One of the other reviewers has mentioned that ...\n",
            "1        A wonderful little production. <br /><br />The...\n",
            "2        I thought this was a wonderful way to spend ti...\n",
            "3        Basically there's a family where a little boy ...\n",
            "4        Petter Mattei's \"Love in the Time of Money\" is...\n",
            "                               ...                        \n",
            "49995    I thought this movie did a down right good job...\n",
            "49996    Bad plot, bad dialogue, bad acting, idiotic di...\n",
            "49997    I am a Catholic taught in parochial elementary...\n",
            "49998    I'm going to have to disagree with the previou...\n",
            "49999    No one expects the Star Trek movies to be high...\n",
            "Name: review, Length: 50000, dtype: object Empty DataFrame\n",
            "Columns: [review, sentiment, normalized_review]\n",
            "Index: []\n",
            "\n",
            "first 10 unigrams of the first sentence\n",
            " [('one',), ('of',), ('the',), ('other',), ('reviewers',), ('has',), ('mentioned',), ('that',), ('after',), ('watching',)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Build a vocabulary based on N-grams"
      ],
      "metadata": {
        "id": "8IAMj75V4gLS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18e828fe",
        "outputId": "aa2b1395-0e99-40f3-adad-1ba3676648ce"
      },
      "source": [
        "# Combine all N-gram lists\n",
        "all_ngrams = unigrams_list + bigrams_list + trigrams_list\n",
        "\n",
        "# Build vocabulary (set of unique N-grams)\n",
        "vocabulary = set(all_ngrams)\n",
        "\n",
        "print(f\"Total number of unique N-grams (vocabulary size): {len(vocabulary)}\")\n",
        "print(\"\\nSample of N-gram vocabulary (first 10 items):\", list(vocabulary)[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique N-grams (vocabulary size): 9324165\n",
            "\n",
            "Sample of N-gram vocabulary (first 10 items): [('on', 'artemisias'), ('masterwork', 'by'), ('music', 'whole'), ('with', 'crimson'), ('line', 'to', 'it'), ('remarkablebr', 'br', 'my'), ('helen', 'prejean', 'but'), ('nothing', 'figure', 'with'), ('art', 'can', 'heal'), ('hudsonand', 'wifepeggy', 'weberhaunted')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Calculate simple N-gram probabilities for a small text sample"
      ],
      "metadata": {
        "id": "7sNO8Bja4h4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist, ConditionalFreqDist\n"
      ],
      "metadata": {
        "id": "a2E-29Cy1LD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"I love programming in Python\",\n",
        "    \"I love programming in Python\",\n",
        "    \"I love programming in Python\",\n",
        "    \"I love programming in Python\",\n",
        "    \"Python is great for data science\",\n",
        "    \"I enjoy learning new algorithms\",\n",
        "    \"Algorithms are the backbone of computer science\",\n",
        "    \"Data science involves statistics and coding\",\n",
        "    \"Coding is a valuable skill\",\n",
        "    \"I love solving problems with code\",\n",
        "    \"Machine learning is fascinating\",\n",
        "    \"Natural language processing is a part of AI\",\n",
        "    \"AI is transforming the world\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "SLOy9wGY4lXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizing sentence into words\n",
        "tokens=[nltk.word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "flat_tokens=[item for sublist in tokens for item in sublist]\n",
        "\n",
        "##genereting bigrams\n",
        "bigrams_list=list(ngrams(flat_tokens,2))\n",
        "print(\"first 10 bigrams\",bigrams_list[:10])\n",
        "\n",
        "#bigramsfrequncy distribution\n",
        "bigrams_freqdist=nltk.FreqDist(bigrams_list)\n",
        "conditional_freqdist=nltk.ConditionalFreqDist(bigrams_list)\n",
        "\n",
        "# Displaying results\n",
        "print(\"Bigrams Frequency Distribution:\")\n",
        "for bigram, freq in bigrams_freqdist.items():\n",
        "    print(f\"{bigram}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQu3kUeC4p2Q",
        "outputId": "3ffff12b-1458-47fe-ff8c-feba54ffb3ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first 10 bigrams [('i', 'love'), ('love', 'programming'), ('programming', 'in'), ('in', 'python'), ('python', 'i'), ('i', 'love'), ('love', 'programming'), ('programming', 'in'), ('in', 'python'), ('python', 'i')]\n",
            "Bigrams Frequency Distribution:\n",
            "('i', 'love'): 5\n",
            "('love', 'programming'): 4\n",
            "('programming', 'in'): 4\n",
            "('in', 'python'): 4\n",
            "('python', 'i'): 3\n",
            "('python', 'python'): 1\n",
            "('python', 'is'): 1\n",
            "('is', 'great'): 1\n",
            "('great', 'for'): 1\n",
            "('for', 'data'): 1\n",
            "('data', 'science'): 2\n",
            "('science', 'i'): 1\n",
            "('i', 'enjoy'): 1\n",
            "('enjoy', 'learning'): 1\n",
            "('learning', 'new'): 1\n",
            "('new', 'algorithms'): 1\n",
            "('algorithms', 'algorithms'): 1\n",
            "('algorithms', 'are'): 1\n",
            "('are', 'the'): 1\n",
            "('the', 'backbone'): 1\n",
            "('backbone', 'of'): 1\n",
            "('of', 'computer'): 1\n",
            "('computer', 'science'): 1\n",
            "('science', 'data'): 1\n",
            "('science', 'involves'): 1\n",
            "('involves', 'statistics'): 1\n",
            "('statistics', 'and'): 1\n",
            "('and', 'coding'): 1\n",
            "('coding', 'coding'): 1\n",
            "('coding', 'is'): 1\n",
            "('is', 'a'): 2\n",
            "('a', 'valuable'): 1\n",
            "('valuable', 'skill'): 1\n",
            "('skill', 'i'): 1\n",
            "('love', 'solving'): 1\n",
            "('solving', 'problems'): 1\n",
            "('problems', 'with'): 1\n",
            "('with', 'code'): 1\n",
            "('code', 'machine'): 1\n",
            "('machine', 'learning'): 1\n",
            "('learning', 'is'): 1\n",
            "('is', 'fascinating'): 1\n",
            "('fascinating', 'natural'): 1\n",
            "('natural', 'language'): 1\n",
            "('language', 'processing'): 1\n",
            "('processing', 'is'): 1\n",
            "('a', 'part'): 1\n",
            "('part', 'of'): 1\n",
            "('of', 'ai'): 1\n",
            "('ai', 'ai'): 1\n",
            "('ai', 'is'): 1\n",
            "('is', 'transforming'): 1\n",
            "('transforming', 'the'): 1\n",
            "('the', 'world'): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Vectorization Techniques\n",
        "\n",
        "\n",
        "• Apply the following to convert text into numerical vectors:\n",
        "\n",
        "● Bag of Words (BoW)\n",
        "\n",
        "● Count Vectorizer\n",
        "\n",
        "• Compare how each method represents the text\n",
        "\n",
        "• Report vocabulary size and sample vector outputs\n",
        "\n",
        "• Discuss limitations (sparsity, lack of semantic meaning)"
      ],
      "metadata": {
        "id": "VjYUTYtQ6V2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Use 5000 reviews for faster processing\n",
        "subset = df[\"review\"][:5000]\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(subset)\n",
        "\n",
        "print(\"Vocabulary Size:\", len(vectorizer.vocabulary_))\n",
        "print(\"\\nSample Vector (first review):\\n\", X[0].toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1IVHlG-8Q2E",
        "outputId": "5fcc20d6-5251-4526-dd45-400a984038fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 38739\n",
            "\n",
            "Sample Vector (first review):\n",
            " [[0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Vectorization Method      | What It Does                        | Vocabulary Size    | Example Output    | Strengths    | Weaknesses                              |\n",
        "| ------------------------- | ----------------------------------- | ------------------ | ----------------- | ------------ | --------------------------------------- |\n",
        "| **BoW (CountVectorizer)** | Converts text to word-count vectors | Varies (thousands) | `[0 2 1 0 3 ...]` | Simple, fast | Sparse matrix, no semantics, order lost |\n"
      ],
      "metadata": {
        "id": "S8CbG6OQ9xat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Text Classification Model**\n",
        "\n",
        "\n",
        "• Build a sentiment classification model using Naive Bayes\n",
        "\n",
        "• Steps:\n",
        "\n",
        "\n",
        "● Split data into train/test\n",
        "\n",
        "● Vectorize the text using BoW or CountVectorizer\n",
        "\n",
        "● Train Multinomial Naive Bayes\n",
        "\n",
        "● Predict sentiment on test data\n",
        "\n",
        "• Tune hyperparameters (optional)"
      ],
      "metadata": {
        "id": "z3IVDMy798m9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)\n",
        "print(df['sentiment'].value_counts())\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "SZGIBz3L9y1N",
        "outputId": "86af6d4d-38b5-484b-8e8c-9087847bb6b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 3)\n",
            "sentiment\n",
            "positive    25000\n",
            "negative    25000\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  ...                                  normalized_review\n",
              "0  One of the other reviewers has mentioned that ...  ...  one of the other reviewers has mentioned that ...\n",
              "1  A wonderful little production. <br /><br />The...  ...  a wonderful little production br br the filmin...\n",
              "2  I thought this was a wonderful way to spend ti...  ...  i thought this was a wonderful way to spend ti...\n",
              "3  Basically there's a family where a little boy ...  ...  basically theres a family where a little boy j...\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  ...  petter matteis love in the time of money is a ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cb7c58a2-5eec-488e-b3e9-598b10f6a21a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>normalized_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>one of the other reviewers has mentioned that ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "      <td>a wonderful little production br br the filmin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "      <td>i thought this was a wonderful way to spend ti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>basically theres a family where a little boy j...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "      <td>petter matteis love in the time of money is a ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb7c58a2-5eec-488e-b3e9-598b10f6a21a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cb7c58a2-5eec-488e-b3e9-598b10f6a21a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cb7c58a2-5eec-488e-b3e9-598b10f6a21a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-794320ec-5258-4918-b383-20e2e9a41745\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-794320ec-5258-4918-b383-20e2e9a41745')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-794320ec-5258-4918-b383-20e2e9a41745 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"\\\"Soul Plane\\\" is a horrible attempt at comedy that only should appeal people with thick skulls, bloodshot eyes and furry pawns. <br /><br />The plot is not only incoherent but also non-existent, acting is mostly sub sub-par with a gang of highly moronic and dreadful characters thrown in for bad measure, jokes are often spotted miles ahead and almost never even a bit amusing. This movie lacks any structure and is full of racial stereotypes that must have seemed old even in the fifties, the only thing it really has going for it is some pretty ladies, but really, if you want that you can rent something from the \\\"Adult\\\" section. OK?<br /><br />I can hardly see anything here to recommend since you'll probably have a lot a better and productive time chasing rats with a sledgehammer or inventing waterproof teabags or whatever.<br /><br />2/10\",\n          \"Guest from the Future tells a fascinating story of time travel, friendship, battle of good and evil -- all with a small budget, child actors, and few special effects. Something for Spielberg and Lucas to learn from. ;) A sixth-grader Kolya \\\"Nick\\\" Gerasimov finds a time machine in the basement of a decrepit building and travels 100 years into the future. He discovers a near-perfect, utopian society where robots play guitars and write poetry, everyone is kind to each other and people enjoy everything technology has to offer. Alice is the daughter of a prominent scientist who invented a device called Mielophone that allows to read minds of humans and animals. The device can be put to both good and bad use, depending on whose hands it falls into. When two evil space pirates from Saturn who want to rule the universe attempt to steal Mielophone, it falls into the hands of 20th century school boy Nick. With the pirates hot on his tracks, he travels back to his time, followed by the pirates, and Alice. Chaos, confusion and funny situations follow as the luckless pirates try to blend in with the earthlings. Alice enrolls in the same school Nick goes to and demonstrates superhuman abilities in PE class. The catch is, Alice doesn't know what Nick looks like, while the pirates do. Also, the pirates are able to change their appearance and turn literally into anyone. (Hmm, I wonder if this is where James Cameron got the idea for Terminator...) Who gets to Nick -- and Mielophone -- first? Excellent plot, non-stop adventures, and great soundtrack. I wish Hollywood made kid movies like this one...\",\n          \"\\\"National Treasure\\\" (2004) is a thoroughly misguided hodge-podge of plot entanglements that borrow from nearly every cloak and dagger government conspiracy clich\\u00e9 that has ever been written. The film stars Nicholas Cage as Benjamin Franklin Gates (how precious is that, I ask you?); a seemingly normal fellow who, for no other reason than being of a lineage of like-minded misguided fortune hunters, decides to steal a 'national treasure' that has been hidden by the United States founding fathers. After a bit of subtext and background that plays laughably (unintentionally) like Indiana Jones meets The Patriot, the film degenerates into one misguided whimsy after another \\u0096 attempting to create a 'Stanley Goodspeed' regurgitation of Nicholas Cage and launch the whole convoluted mess forward with a series of high octane, but disconnected misadventures.<br /><br />The relevancy and logic to having George Washington and his motley crew of patriots burying a king's ransom someplace on native soil, and then, going through the meticulous plan of leaving clues scattered throughout U.S. currency art work, is something that director Jon Turteltaub never quite gets around to explaining. Couldn't Washington found better usage for such wealth during the start up of the country? Hence, we are left with a mystery built on top of an enigma that is already on shaky ground by the time Ben appoints himself the new custodian of this untold wealth. Ben's intentions are noble \\u0096 if confusing. He's set on protecting the treasure. For who and when?\\u0085your guess is as good as mine.<br /><br />But there are a few problems with Ben's crusade. First up, his friend, Ian Holmes (Sean Bean) decides that he can't wait for Ben to make up his mind about stealing the Declaration of Independence from the National Archives (oh, yeah \\u0096 brilliant idea!). Presumably, the back of that famous document holds the secret answer to the ultimate fortune. So Ian tries to kill Ben. The assassination attempt is, of course, unsuccessful, if overly melodramatic. It also affords Ben the opportunity to pick up, and pick on, the very sultry curator of the archives, Abigail Chase (Diane Kruger). She thinks Ben is clearly a nut \\u0096 at least at the beginning. But true to action/romance form, Abby's resolve melts quicker than you can say, \\\"is that the Hope Diamond?\\\" The film moves into full X-File-ish mode, as the FBI, mistakenly believing that Ben is behind the theft, retaliate in various benign ways that lead to a multi-layering of action sequences reminiscent of Mission Impossible meets The Fugitive. Honestly, don't those guys ever get 'intelligence' information that is correct? In the final analysis, \\\"National Treasure\\\" isn't great film making, so much as it's a patchwork rehash of tired old bits from other movies, woven together from scraps, the likes of which would make IL' Betsy Ross blush.<br /><br />The Buena Vista DVD delivers a far more generous treatment than this film is deserving of. The anamorphic widescreen picture exhibits a very smooth and finely detailed image with very rich colors, natural flesh tones, solid blacks and clean whites. The stylized image is also free of blemishes and digital enhancements. The audio is 5.1 and delivers a nice sonic boom to your side and rear speakers with intensity and realism. Extras include a host of promotional junket material that is rather deep and over the top in its explanation of how and why this film was made. If only, as an audience, we had had more clarification as to why Ben and co. were chasing after an illusive treasure, this might have been one good flick. Extras conclude with the theatrical trailer, audio commentary and deleted scenes. Not for the faint-hearted \\u0096 just the thick-headed.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"normalized_review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49581,\n        \"samples\": [\n          \"poorly done political actioner badly photographed acted and directed every single scene is underlighted including those very few that are shot during the daytime it doesnt matter what the location is at an important conference in the white house no lights are on and the only available lighting is a gloomy blue that is filtered through a few windows the primier of china conducts an earthshattering phone conversation under conditions of such intense chiaroscuro that he should be contemplating a bust of homer in a rembrandt painting honest its as if he had a tiny spotlight on his face and was otherwise in total darkness the slow motion deaths are by now obligatory in any illthoughtout moviebr br roy scheider and maria conchita alonzo do well by their roles but scheider is rarely on screen the other performances are dismissable there is a pretty oriental woman in a short tight skirt who totes a gun and is right out of a bond movie whos accent suggests a childhood spent in basset nebraska and who should have remained the model she probably started out as whoever plays the surviving secret service agent aboard the cruise ship was probably picked for the part because he looked most like johnny depp not because of any display of talent the chinese villains representing both taiwan and mainland china hiss and grin as they threaten the heroes br br the script is pretty awful recycled from other better films there is a lot of shooting aboard the ship and practically everyone winds up mincemeat two thirds of the way through the ship explodes into the expected series of fireballs then the movie splits into two related parts part one another shootout this time in a waterfront warehouse part two an exchange between the vice president now acting president and the oily chinese premiere lifted out of both dr strangelove and fail safe we unwittingly launch our missiles they launch theirs in retaliation we cannot convince them that our launch was accidental even though we offer to help them destroy our own missiles there is even the george c scott walter matthau general who argues that their nucular armory cant match ours so we should hit them with everything weve got more fireballs br br the end comes none too soon\",\n          \"not good mostly because you dont give a damn about what happens to all these people some comments  1 i am tired of seeing governesses who never talk to their pupils never teach them anything and take a tired and annoyed look whenever the said pupil who of course has been won over in the space of 4 seconds says something 2 fine so rosina has a father complex and therefore is attracted to her employer but charles is completely different in all aspects from her father  if anything henry is much closer as a sensual exalted person 3 how could you ever believe that she would be more attracted to tom wilkinson than to rhys meyer 4 hard to believe if she had been in fact raised as a deeply religious girl that she would be so careless about sleeping with a gentile after knowing him for 5 minutesbr br some good things about the film  at least she didnt end up pregnant not knowing who the father was the whole description of life in the jewish community in london is good\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils import shuffle\n",
        "import joblib\n",
        "df = shuffle(df, random_state=42).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "22UZpfIJ-K1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  text=str(text)\n",
        "  #removing html tags etc\n",
        "  text = re.sub(r\"<.*?>\", \" \", text)\n",
        "  #removing special characters\n",
        "  text=re.sub(r\"\\s+\", \"\", text)\n",
        "  return text\n",
        "\n",
        "df['review'] = df['review'].apply(clean_text)"
      ],
      "metadata": {
        "id": "od5cYvHiA8zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Train/Test split\n",
        "X = df['review']\n",
        "y = df['sentiment']\n",
        "y_binary = (y == 'positive').astype(int)"
      ],
      "metadata": {
        "id": "FXbIzsIbC1eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_binary, test_size=0.20, random_state=42, stratify=y_binary\n",
        ")\n",
        "\n",
        "print(\"Train size:\", X_train.shape[0])\n",
        "print(\"Test size:\", X_test.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5azMhiiC6ac",
        "outputId": "0fa062ec-6173-43af-9a3d-279807184b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 40000\n",
            "Test size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " ##Build pipeline: CountVectorizer -> MultinomialNB\n",
        "pipeline = Pipeline([\n",
        "    ('vect', CountVectorizer(stop_words='english', max_features=50000)),  # adjust max_features\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Train\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
        "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jItJJ4LeC9kW",
        "outputId": "3a086529-7ae0-4239-b9c4-2135d00db678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7391\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7132    0.7998    0.7540      5000\n",
            "           1     0.7721    0.6784    0.7222      5000\n",
            "\n",
            "    accuracy                         0.7391     10000\n",
            "   macro avg     0.7427    0.7391    0.7381     10000\n",
            "weighted avg     0.7427    0.7391    0.7381     10000\n",
            "\n",
            "\n",
            "Confusion matrix:\n",
            " [[3999 1001]\n",
            " [1608 3392]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save whole pipeline (vectorizer + classifier)\n",
        "joblib.dump(pipeline, \"nb_countvec_pipeline.joblib\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrT7AvA7DFcj",
        "outputId": "11513709-88e8-46ab-dc33-4660003a0e76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nb_countvec_pipeline.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Predictions already made earlier:\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Precision, Recall, F1-score\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR9RMxTTDNiu",
        "outputId": "9b8a14b1-337b-4107-ab58-9300043c494b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7391\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7132    0.7998    0.7540      5000\n",
            "           1     0.7721    0.6784    0.7222      5000\n",
            "\n",
            "    accuracy                         0.7391     10000\n",
            "   macro avg     0.7427    0.7391    0.7381     10000\n",
            "weighted avg     0.7427    0.7391    0.7381     10000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[3999 1001]\n",
            " [1608 3392]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **6. Reflection**\n",
        "\n",
        "**1. Stemming/Lemmatization:**\n",
        "Reduced vocabulary and sparsity slightly; minor improvement in consistency.\n",
        "\n",
        "**2. N-grams:**\n",
        "Bigrams captured phrases like “not good” and improved performance slightly; increased vocabulary size.\n",
        "\n",
        "**3. Vectorization (BoW/CountVectorizer):**\n",
        "CountVectorizer worked efficiently; vocabulary size and n-grams affected sparsity and detail captured.\n",
        "\n",
        "**4. Why Naive Bayes works:**\n",
        "Handles high-dimensional sparse text well, fast, and performs strongly despite feature independence assumption.\n",
        "\n",
        "**5. Possible Improvements:**\n",
        "TF-IDF, word embeddings, deep learning models (LSTM, BERT), and better hyperparameter tuning could boost performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "8qLxYmngFbtj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t1OZFv7vGI-r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}